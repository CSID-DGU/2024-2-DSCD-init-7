{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\82102\\miniconda3\\envs\\hyunjae\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\82102\\miniconda3\\envs\\hyunjae\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "\n",
    "# MySQL 서버에 연결\n",
    "conn = mysql.connector.connect(\n",
    "    host='127.0.0.1',      # 호스트 이름\n",
    "    user='root',       # MySQL 사용자 이름\n",
    "    password='hj010701',   # MySQL 사용자 비밀번호\n",
    "    database='employee'  # 연결할 데이터베이스 이름\n",
    ")\n",
    "\n",
    "# 커서 생성\n",
    "cursor = conn.cursor()\n",
    "sql_query = \"SELECT * FROM member_assign_50to100\"\n",
    "cursor.execute(sql_query)\n",
    "\n",
    "result = cursor.fetchall()\n",
    "\n",
    "column_names = [i[0] for i in cursor.description]\n",
    "\n",
    "member_based_okr_assignments = pd.DataFrame(result, columns=column_names)\n",
    "\n",
    "################################################################################\n",
    "sql_query = \"SELECT * FROM okr_30to60\"\n",
    "cursor.execute(sql_query)\n",
    "\n",
    "result = cursor.fetchall()\n",
    "\n",
    "column_names = [i[0] for i in cursor.description]\n",
    "\n",
    "okr_df = pd.DataFrame(result, columns=column_names)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "sql_query = '''\n",
    "SELECT *\n",
    "FROM member_assign_50to100\n",
    "JOIN okr_30to60 \n",
    "ON okr_30to60.OKR_NUM IN (member_assign_50to100.project1, member_assign_50to100.project2, member_assign_50to100.project3);\n",
    "'''\n",
    "cursor.execute(sql_query)\n",
    "\n",
    "result = cursor.fetchall()\n",
    "\n",
    "column_names = [i[0] for i in cursor.description]\n",
    "\n",
    "member_okr = pd.DataFrame(result, columns=column_names)\n",
    "\n",
    "\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "df = member_okr\n",
    "\n",
    "# 'Member' 컬럼 기준으로 오름차순 정렬\n",
    "df_sorted = df.sort_values(by='Member', ascending=True)\n",
    "\n",
    "# 데이터를 저장할 리스트 초기화\n",
    "flattened_data = []\n",
    "\n",
    "# 전체 열 개수 확인\n",
    "num_columns = df_sorted.shape[1]\n",
    "\n",
    "# 데이터 처리 및 열 범위에 따른 조건 설정\n",
    "for i in range(50):\n",
    "    # 열 범위 설정\n",
    "    if 0 <= i < 10 or 50<= i <60:\n",
    "        selected_columns = [12] + [col for col in range(14, 20) if col < num_columns]\n",
    "    elif 10 <= i < 20 or 60<= i <70:\n",
    "        selected_columns = [12] + [col for col in range(20, 26) if col < num_columns]\n",
    "    elif 20 <= i < 30 or 70<= i <80:\n",
    "        selected_columns = [12] + [col for col in range(26, 32) if col < num_columns]\n",
    "    elif 30 <= i < 40 or 80<= i <90:\n",
    "        selected_columns = [12] + [col for col in range(32, 38) if col < num_columns]\n",
    "    elif 40 <= i < 50 or 90<= i <100:\n",
    "        selected_columns = [12] + [col for col in range(38, 44) if col < num_columns]\n",
    "\n",
    "    # 선택된 열의 첫 번째 데이터 가져오기\n",
    "    first_row_data = df_sorted.iloc[3 * i : 3 * (i + 1), selected_columns[0]].T.tolist()\n",
    "\n",
    "    # 나머지 열 데이터 가져와 병합\n",
    "    other_data = df_sorted.iloc[3 * i : 3 * (i + 1), selected_columns[1:]].values.flatten().tolist()\n",
    "\n",
    "    # 데이터 조합\n",
    "    combined_data = [i + 1, np.nan] + first_row_data + other_data\n",
    "    flattened_data.append(combined_data)\n",
    "\n",
    "# 컬럼명 설정 (최대 길이에 맞게 조정)\n",
    "column_names = [\n",
    "    'member', 'N_OKR', 'pr1_score', 'pr2_score', 'pr3_score', \n",
    "    'pr1_1', 'pr1_2', 'pr1_3', 'pr1_4', 'pr1_5', 'pr1_6', \n",
    "    'pr2_1', 'pr2_2', 'pr2_3', 'pr2_4', 'pr2_5', 'pr2_6', \n",
    "    'pr3_1', 'pr3_2', 'pr3_3', 'pr3_4', 'pr3_5', 'pr3_6'\n",
    "]\n",
    "max_length = max(len(row) for row in flattened_data)\n",
    "adjusted_column_names = column_names[:max_length]\n",
    "\n",
    "# 새로운 DataFrame 생성 및 CSV로 저장\n",
    "data = pd.DataFrame(flattened_data, columns=adjusted_column_names)\n",
    "\n",
    "data=data.iloc[:,1:]\n",
    "\n",
    "\n",
    "objectives = okr_df['Objective']\n",
    "posted_OKR=okr_df['OKR_NUM']\n",
    "\n",
    "posted_OKR=posted_OKR.str.replace(\"OKR_\", \"\").astype(float)\n",
    "score=okr_df['Objective Score']\n",
    "\n",
    "def generate_combinations_3d(data, num_parts=5):\n",
    "    # Convert data to numpy array for easier manipulation\n",
    "    data_values = data.values\n",
    "    \n",
    "    # Split data into equal parts\n",
    "    part_size = len(data_values) // num_parts\n",
    "    parts = [data_values[i * part_size:(i + 1) * part_size] for i in range(num_parts)]\n",
    "    \n",
    "    # Generate all possible combinations from the parts (row-wise combinations)\n",
    "    combinations = list(itertools.product(*parts))\n",
    "    \n",
    "    # Convert combinations to a 3D numpy array\n",
    "    combinations_3d = np.array(combinations)\n",
    "    \n",
    "    return combinations_3d\n",
    "\n",
    "\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def get_similarities(n_okr, objectives):\n",
    "    # n_okr와 각 objective에 대해 임베딩 생성\n",
    "    n_okr_embedding = model.encode(n_okr, convert_to_tensor=True)\n",
    "    similarities = []\n",
    "\n",
    "    for objective in objectives:\n",
    "        obj_embedding = model.encode(objective, convert_to_tensor=True)\n",
    "        similarity = torch.nn.functional.cosine_similarity(n_okr_embedding, obj_embedding, dim=-1).item()\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    return similarities\n",
    "\n",
    "def calculate_weighted_scores(n_okr): \n",
    "    \n",
    "    df = member_okr\n",
    "\n",
    "    # 각 멤버의 유사도 계산 결과를 저장할 리스트\n",
    "    weighted_sums = []\n",
    "\n",
    "    # 각 멤버별로 데이터를 그룹화\n",
    "    grouped_df = df.groupby('Member')\n",
    "\n",
    "    for member, group in grouped_df:\n",
    "        objectives = group['Objective'].tolist()\n",
    "        objective_scores = group['Objective Score'].tolist()\n",
    "\n",
    "        # 유사도 계산 (멤버별 3개의 okr에 대한 유사도)\n",
    "        similarities = get_similarities(n_okr, objectives)\n",
    "        \n",
    "        #print(similarities)\n",
    "\n",
    "        total_weighted_score = 0\n",
    "        valid_count = 0\n",
    "\n",
    "        # 유사도와 Objective Score를 곱해 가중합 계산\n",
    "        for similarity, objective_score, objective in zip(similarities, objective_scores, objectives):\n",
    "            if objective != n_okr:  # objective가 같은 경우 제외\n",
    "                weighted_score = similarity * objective_score\n",
    "                total_weighted_score += weighted_score\n",
    "                valid_count += 1\n",
    "\n",
    "        if valid_count > 0:\n",
    "            weighted_sums.append((member, total_weighted_score / valid_count))\n",
    "        else:\n",
    "            weighted_sums.append((member, 0))  # 모든 유사도가 1에 가까운 경우\n",
    "\n",
    "    return weighted_sums\n",
    "\n",
    "all_data_f = []\n",
    "\n",
    "# 사용자가 입력한 n_okr, posted, label 값\n",
    "n_okr_input = \"Your custom OKR here\"  # 여기에 입력하고 싶은 OKR 문장을 넣으세요\n",
    "posted_input = 61.0  # OKR 번호\n",
    "label_input = np.nan\n",
    "\n",
    "# 루프: 기존 30개의 OKR 자리를 사용자가 입력한 n_okr로 대체\n",
    "for _ in range(len(objectives)):  # objectives의 길이를 기준으로 반복\n",
    "    # Step 2: calculate_weighted_scores 함수 사용하여 weighted_sums 계산\n",
    "    weighted_sums = calculate_weighted_scores(n_okr_input)[:50]  # 멤버 수는 최대 50개로 제한\n",
    "\n",
    "    # Step 3: weighted_sums에서 두 번째 값을 추출\n",
    "    weighted_values = [value[1] for value in weighted_sums]\n",
    "\n",
    "    # Step 4: data의 첫 번째 열(0열)에 weighted_values 추가\n",
    "    weighted_array = np.array(weighted_values)\n",
    "\n",
    "    if data.shape[0] == len(weighted_values):\n",
    "        data.iloc[:, 0] = weighted_array  # pandas의 iloc 사용하여 첫 번째 열에 할당\n",
    "        data[\"member\"] = data.index.astype(int)\n",
    "        data[\"posted\"] = posted_input  # 사용자 입력값으로 업데이트\n",
    "        data['label'] = label_input  # 사용자 입력값으로 업데이트\n",
    "    else:\n",
    "        print(f\"샘플 수가 일치하지 않습니다. data 행 수: {data.shape[0]}, weighted_values 길이: {len(weighted_values)}\")\n",
    "    \n",
    "    # Generate combinations based on the updated data\n",
    "    data_3d = generate_combinations_3d(data.iloc[:, :], num_parts=5)\n",
    "\n",
    "    # data_f 계산\n",
    "    data_f = np.concatenate((data_3d[:, :, 0:1], data_3d[:, :, 4:]), axis=2)\n",
    "\n",
    "    # data_f를 dim=0에서 쌓기 위해 리스트에 저장\n",
    "    all_data_f.append(data_f)\n",
    "\n",
    "# Step 5: dim=0에서 모든 data_f 연결\n",
    "final_data_f = np.concatenate(all_data_f, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_19272\\495916134.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('best_model_weights.pth', map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TeamTransformer:\n\tsize mismatch for transformer_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([57, 19]) from checkpoint, the shape in current model is torch.Size([66, 22]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([57]) from checkpoint, the shape in current model is torch.Size([66]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([19, 19]) from checkpoint, the shape in current model is torch.Size([22, 22]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([64, 19]) from checkpoint, the shape in current model is torch.Size([64, 22]).\n\tsize mismatch for transformer_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([19, 64]) from checkpoint, the shape in current model is torch.Size([22, 64]).\n\tsize mismatch for transformer_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([57, 19]) from checkpoint, the shape in current model is torch.Size([66, 22]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([57]) from checkpoint, the shape in current model is torch.Size([66]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([19, 19]) from checkpoint, the shape in current model is torch.Size([22, 22]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.1.linear1.weight: copying a param with shape torch.Size([64, 19]) from checkpoint, the shape in current model is torch.Size([64, 22]).\n\tsize mismatch for transformer_encoder.layers.1.linear2.weight: copying a param with shape torch.Size([19, 64]) from checkpoint, the shape in current model is torch.Size([22, 64]).\n\tsize mismatch for transformer_encoder.layers.1.linear2.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.1.norm1.weight: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.1.norm1.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.1.norm2.weight: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.1.norm2.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([57, 19]) from checkpoint, the shape in current model is torch.Size([66, 22]).\n\tsize mismatch for transformer_encoder.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([57]) from checkpoint, the shape in current model is torch.Size([66]).\n\tsize mismatch for transformer_encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([19, 19]) from checkpoint, the shape in current model is torch.Size([22, 22]).\n\tsize mismatch for transformer_encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.2.linear1.weight: copying a param with shape torch.Size([64, 19]) from checkpoint, the shape in current model is torch.Size([64, 22]).\n\tsize mismatch for transformer_encoder.layers.2.linear2.weight: copying a param with shape torch.Size([19, 64]) from checkpoint, the shape in current model is torch.Size([22, 64]).\n\tsize mismatch for transformer_encoder.layers.2.linear2.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.2.norm1.weight: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.2.norm1.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.2.norm2.weight: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.2.norm2.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for fc_out.weight: copying a param with shape torch.Size([1, 19]) from checkpoint, the shape in current model is torch.Size([1, 22]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 저장된 가중치 로드\u001b[39;00m\n\u001b[0;32m     24\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_weights.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 25\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 평가 모드로 전환\u001b[39;00m\n\u001b[0;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\82102\\miniconda3\\envs\\hyunjae\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2580\u001b[0m             ),\n\u001b[0;32m   2581\u001b[0m         )\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2587\u001b[0m         )\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TeamTransformer:\n\tsize mismatch for transformer_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([57, 19]) from checkpoint, the shape in current model is torch.Size([66, 22]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([57]) from checkpoint, the shape in current model is torch.Size([66]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([19, 19]) from checkpoint, the shape in current model is torch.Size([22, 22]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([64, 19]) from checkpoint, the shape in current model is torch.Size([64, 22]).\n\tsize mismatch for transformer_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([19, 64]) from checkpoint, the shape in current model is torch.Size([22, 64]).\n\tsize mismatch for transformer_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([57, 19]) from checkpoint, the shape in current model is torch.Size([66, 22]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([57]) from checkpoint, the shape in current model is torch.Size([66]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([19, 19]) from checkpoint, the shape in current model is torch.Size([22, 22]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.1.linear1.weight: copying a param with shape torch.Size([64, 19]) from checkpoint, the shape in current model is torch.Size([64, 22]).\n\tsize mismatch for transformer_encoder.layers.1.linear2.weight: copying a param with shape torch.Size([19, 64]) from checkpoint, the shape in current model is torch.Size([22, 64]).\n\tsize mismatch for transformer_encoder.layers.1.linear2.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.1.norm1.weight: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.1.norm1.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.1.norm2.weight: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.1.norm2.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([57, 19]) from checkpoint, the shape in current model is torch.Size([66, 22]).\n\tsize mismatch for transformer_encoder.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([57]) from checkpoint, the shape in current model is torch.Size([66]).\n\tsize mismatch for transformer_encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([19, 19]) from checkpoint, the shape in current model is torch.Size([22, 22]).\n\tsize mismatch for transformer_encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.2.linear1.weight: copying a param with shape torch.Size([64, 19]) from checkpoint, the shape in current model is torch.Size([64, 22]).\n\tsize mismatch for transformer_encoder.layers.2.linear2.weight: copying a param with shape torch.Size([19, 64]) from checkpoint, the shape in current model is torch.Size([22, 64]).\n\tsize mismatch for transformer_encoder.layers.2.linear2.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.2.norm1.weight: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.2.norm1.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.2.norm2.weight: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for transformer_encoder.layers.2.norm2.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([22]).\n\tsize mismatch for fc_out.weight: copying a param with shape torch.Size([1, 19]) from checkpoint, the shape in current model is torch.Size([1, 22])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from algorithm import TeamTransformer  # TeamTransformer 클래스 불러오기\n",
    "\n",
    "# 모델 파라미터 설정 (학습 시와 동일해야 함)\n",
    "embedding_dim = 19\n",
    "seq_len = 5\n",
    "output_dim = 1\n",
    "n_heads = 1\n",
    "n_layers = 3\n",
    "hidden_dim = 64\n",
    "dropout_rate = 0.2\n",
    "\n",
    "# 모델 초기화\n",
    "model = TeamTransformer(\n",
    "    embedding_dim=embedding_dim,\n",
    "    n_heads=n_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_layers=n_layers,\n",
    "    output_dim=output_dim,\n",
    "    dropout_rate=dropout_rate,\n",
    ")\n",
    "\n",
    "# 저장된 가중치 로드\n",
    "state_dict = torch.load('best_model_weights.pth', map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# 평가 모드로 전환\n",
    "model.eval()\n",
    "\n",
    "# 예측 수행\n",
    "input_tensor = torch.tensor(final_data_f, dtype=torch.float32)  # 입력 데이터 변환\n",
    "with torch.no_grad():\n",
    "    predictions, _ = model(input_tensor)  # 모델 예측 수행\n",
    "\n",
    "predicted_labels = predictions.numpy()\n",
    "final_data_f[:, :, -1] = predicted_labels\n",
    "final_data_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Temp\\ipykernel_8232\\2319489034.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  trained_model = torch.load('best_model_weights.pth', map_location=torch.device('cpu'))  # 모델 경로를 지정하세요\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 학습된 모델 불러오기\u001b[39;00m\n\u001b[0;32m      2\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_weights.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))  \u001b[38;5;66;03m# 모델 경로를 지정하세요\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m trained_model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# 평가 모드로 전환\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# final_data_f에서 예측 수행\u001b[39;00m\n\u001b[0;32m      6\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(final_data_f, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# 모델 입력 형태로 변환\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "# 학습된 모델 불러오기\n",
    "trained_model = torch.load('best_model_weights.pth', map_location=torch.device('cpu'))  # 모델 경로를 지정하세요\n",
    "trained_model.eval()  # 평가 모드로 전환\n",
    "\n",
    "# final_data_f에서 예측 수행\n",
    "input_tensor = torch.tensor(final_data_f, dtype=torch.float32)  # 모델 입력 형태로 변환\n",
    "\n",
    "# 예측 수행\n",
    "with torch.no_grad():\n",
    "    predictions = trained_model(input_tensor)  # 모델 예측\n",
    "    predicted_labels = torch.argmax(predictions, dim=-1).numpy()  # 예측된 라벨 추출\n",
    "\n",
    "# 예측된 라벨로 마지막 열(22번째)을 교체\n",
    "final_data_f[:, :, -1] = predicted_labels[:, None]  # 마지막 열을 라벨로 교체\n",
    "\n",
    "# 결과 확인\n",
    "print(\"22번째 열(마지막 열)이 라벨로 교체된 데이터셋:\\n\", final_data_f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 5, 22)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 7.93741633,  1.        ,  3.        , ...,  0.        ,\n",
       "         61.        ,         nan],\n",
       "        [11.63435521,  5.        ,  3.        , ..., 10.        ,\n",
       "         61.        ,         nan],\n",
       "        [ 7.1600523 ,  5.        ,  4.        , ..., 20.        ,\n",
       "         61.        ,         nan],\n",
       "        [ 9.45510848,  3.        ,  1.        , ..., 30.        ,\n",
       "         61.        ,         nan],\n",
       "        [ 4.64377764,  1.        ,  2.        , ..., 40.        ,\n",
       "         61.        ,         nan]],\n",
       "\n",
       "       [[ 7.93741633,  1.        ,  3.        , ...,  0.        ,\n",
       "         61.        ,         nan],\n",
       "        [11.63435521,  5.        ,  3.        , ..., 10.        ,\n",
       "         61.        ,         nan],\n",
       "        [ 7.1600523 ,  5.        ,  4.        , ..., 20.        ,\n",
       "         61.        ,         nan],\n",
       "        [ 9.45510848,  3.        ,  1.        , ..., 30.        ,\n",
       "         61.        ,         nan],\n",
       "        [ 5.31392879,  2.        ,  3.        , ..., 41.        ,\n",
       "         61.        ,         nan]],\n",
       "\n",
       "       [[ 7.93741633,  1.        ,  3.        , ...,  0.        ,\n",
       "         61.        ,         nan],\n",
       "        [11.63435521,  5.        ,  3.        , ..., 10.        ,\n",
       "         61.        ,         nan],\n",
       "        [ 7.1600523 ,  5.        ,  4.        , ..., 20.        ,\n",
       "         61.        ,         nan],\n",
       "        [ 9.45510848,  3.        ,  1.        , ..., 30.        ,\n",
       "         61.        ,         nan],\n",
       "        [ 6.30878486,  5.        ,  2.        , ..., 42.        ,\n",
       "         61.        ,         nan]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[11.54815364,  4.        ,  4.        , ...,  9.        ,\n",
       "         61.        ,         nan],\n",
       "        [ 0.52516785,  1.        ,  4.        , ..., 19.        ,\n",
       "         61.        ,         nan],\n",
       "        [ 6.3023066 ,  5.        ,  2.        , ..., 29.        ,\n",
       "         61.        ,         nan],\n",
       "        [ 7.0440215 ,  1.        ,  1.        , ..., 39.        ,\n",
       "         61.        ,         nan],\n",
       "        [11.53365401,  1.        ,  2.        , ..., 47.        ,\n",
       "         61.        ,         nan]],\n",
       "\n",
       "       [[11.54815364,  4.        ,  4.        , ...,  9.        ,\n",
       "         61.        ,         nan],\n",
       "        [ 0.52516785,  1.        ,  4.        , ..., 19.        ,\n",
       "         61.        ,         nan],\n",
       "        [ 6.3023066 ,  5.        ,  2.        , ..., 29.        ,\n",
       "         61.        ,         nan],\n",
       "        [ 7.0440215 ,  1.        ,  1.        , ..., 39.        ,\n",
       "         61.        ,         nan],\n",
       "        [ 9.65773395,  5.        ,  1.        , ..., 48.        ,\n",
       "         61.        ,         nan]],\n",
       "\n",
       "       [[11.54815364,  4.        ,  4.        , ...,  9.        ,\n",
       "         61.        ,         nan],\n",
       "        [ 0.52516785,  1.        ,  4.        , ..., 19.        ,\n",
       "         61.        ,         nan],\n",
       "        [ 6.3023066 ,  5.        ,  2.        , ..., 29.        ,\n",
       "         61.        ,         nan],\n",
       "        [ 7.0440215 ,  1.        ,  1.        , ..., 39.        ,\n",
       "         61.        ,         nan],\n",
       "        [ 5.57958124,  4.        ,  2.        , ..., 49.        ,\n",
       "         61.        ,         nan]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyunjae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
